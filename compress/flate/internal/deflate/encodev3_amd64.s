// Code generated by command: go run main.go. DO NOT EDIT.

#include "textflag.h"

DATA max_write_d<>+0(SB)/4, $0x0000001c
DATA max_write_d<>+4(SB)/4, $0x0000001d
DATA max_write_d<>+8(SB)/4, $0x0000001f
DATA max_write_d<>+12(SB)/4, $0x00000020
DATA max_write_d<>+16(SB)/4, $0x0000001c
DATA max_write_d<>+20(SB)/4, $0x0000001d
DATA max_write_d<>+24(SB)/4, $0x0000001f
DATA max_write_d<>+28(SB)/4, $0x00000020
GLOBL max_write_d<>(SB), RODATA, $32

DATA min_write_mask<>+0(SB)/8, $0x0000000000000000
DATA min_write_mask<>+8(SB)/8, $0x0000000000000000
DATA min_write_mask<>+16(SB)/8, $0x00000000000000ff
DATA min_write_mask<>+24(SB)/8, $0x0000000000000000
GLOBL min_write_mask<>(SB), RODATA, $32

DATA offset_mask<>+0(SB)/8, $0x0000000000000007
DATA offset_mask<>+8(SB)/8, $0x0000000000000000
DATA offset_mask<>+16(SB)/8, $0x0000000000000000
DATA offset_mask<>+24(SB)/8, $0x0000000000000000
GLOBL offset_mask<>(SB), RODATA, $32

DATA q_64<>+0(SB)/8, $0x0000000000000040
DATA q_64<>+8(SB)/8, $0x0000000000000000
DATA q_64<>+16(SB)/8, $0x0000000000000040
DATA q_64<>+24(SB)/8, $0x0000000000000000
GLOBL q_64<>(SB), RODATA, $32

DATA lit_mask<>+0(SB)/4, $0x000003ff
DATA lit_mask<>+4(SB)/4, $0x000003ff
DATA lit_mask<>+8(SB)/4, $0x000003ff
DATA lit_mask<>+12(SB)/4, $0x000003ff
DATA lit_mask<>+16(SB)/4, $0x000003ff
DATA lit_mask<>+20(SB)/4, $0x000003ff
DATA lit_mask<>+24(SB)/4, $0x000003ff
DATA lit_mask<>+28(SB)/4, $0x000003ff
GLOBL lit_mask<>(SB), RODATA, $32

DATA dist_mask<>+0(SB)/4, $0x000001ff
DATA dist_mask<>+4(SB)/4, $0x000001ff
DATA dist_mask<>+8(SB)/4, $0x000001ff
DATA dist_mask<>+12(SB)/4, $0x000001ff
DATA dist_mask<>+16(SB)/4, $0x000001ff
DATA dist_mask<>+20(SB)/4, $0x000001ff
DATA dist_mask<>+24(SB)/4, $0x000001ff
DATA dist_mask<>+28(SB)/4, $0x000001ff
GLOBL dist_mask<>(SB), RODATA, $32

DATA lit_token_mask<>+0(SB)/4, $0x00ffffff
DATA lit_token_mask<>+4(SB)/4, $0x00ffffff
DATA lit_token_mask<>+8(SB)/4, $0x00ffffff
DATA lit_token_mask<>+12(SB)/4, $0x00ffffff
DATA lit_token_mask<>+16(SB)/4, $0x00ffffff
DATA lit_token_mask<>+20(SB)/4, $0x00ffffff
DATA lit_token_mask<>+24(SB)/4, $0x00ffffff
DATA lit_token_mask<>+28(SB)/4, $0x00ffffff
GLOBL lit_token_mask<>(SB), RODATA, $32

DATA eb_token_mask<>+0(SB)/4, $0x000000ff
DATA eb_token_mask<>+4(SB)/4, $0x000000ff
DATA eb_token_mask<>+8(SB)/4, $0x000000ff
DATA eb_token_mask<>+12(SB)/4, $0x000000ff
DATA eb_token_mask<>+16(SB)/4, $0x000000ff
DATA eb_token_mask<>+20(SB)/4, $0x000000ff
DATA eb_token_mask<>+24(SB)/4, $0x000000ff
DATA eb_token_mask<>+28(SB)/4, $0x000000ff
GLOBL eb_token_mask<>(SB), RODATA, $32

DATA bytes_mask<>+0(SB)/8, $0x00000000000000ff
DATA bytes_mask<>+8(SB)/8, $0x0000000000000000
DATA bytes_mask<>+16(SB)/8, $0x00000000000000ff
DATA bytes_mask<>+24(SB)/8, $0x0000000000000000
GLOBL bytes_mask<>(SB), RODATA, $32

// func encodeTokensArchV3(hist *histogram, tokens []token, buf *BitBuf) int
// Requires: AVX, AVX2, BMI2
TEXT Â·encodeTokensArchV3(SB), NOSPLIT, $0-48
        // table = hist
        MOVQ hist+0(FP), AX
        MOVQ buf+32(FP), CX

        // output = buf.output
        MOVQ (CX), DX

        // outputEnd = output + len(buf.output) 
        MOVQ       8(CX), BX
        CMPQ       BX, $0x00000018
        JBE        skip
        ADDQ       DX, BX
        SUBQ       $0x00000018, BX
        MOVQ       24(CX), SI
        ADDQ       SI, DX
        MOVQ       32(CX), SI
        MOVQ       40(CX), CX
        MOVQ       tokens_base+8(FP), DI
        MOVQ       tokens_len+16(FP), R8
        CMPQ       R8, $0x00000040
        JBE        skip
        LEAQ       -64(DI)(R8*4), R8
        VPCMPEQQ   Y7, Y7, Y7
        VMOVDQU    (DI), Y10
        VPAND      lit_mask<>+0(SB), Y10, Y6
        VPGATHERDD Y7, 124(AX)(Y6*4), Y8
        VPCMPEQQ   Y7, Y7, Y7
        VPSRLD     $0x0a, Y10, Y6
        VPAND      dist_mask<>+0(SB), Y6, Y6
        VPGATHERDD Y7, (AX)(Y6*4), Y9
        VMOVQ      SI, X11
        VMOVQ      CX, X12
        VMOVDQA    offset_mask<>+0(SB), Y13

main_loop:
        VPSRLD       $0x18, Y8, Y1
        VPAND        lit_token_mask<>+0(SB), Y8, Y0
        VPBLENDW     $0x55, Y9, Y11, Y2
        VPSRLD       $0x18, Y9, Y3
        VPSRLD       $0x10, Y9, Y5
        VPAND        eb_token_mask<>+0(SB), Y5, Y5
        VPSRLD       $0x13, Y10, Y4
        CMPQ         DX, BX
        JA           main_loop_exit
        ADDQ         $0x00000020, DI
        VPCMPEQQ     Y7, Y7, Y7
        VMOVDQU      (DI), Y10
        VPAND        lit_mask<>+0(SB), Y10, Y6
        VPGATHERDD   Y7, 124(AX)(Y6*4), Y8
        VPCMPEQQ     Y7, Y7, Y7
        VPSRLD       $0x0a, Y10, Y6
        VPAND        dist_mask<>+0(SB), Y6, Y6
        VPGATHERDD   Y7, (AX)(Y6*4), Y9
        VPSLLVD      Y3, Y4, Y4
        VPXOR        Y4, Y2, Y2
        VPADDD       Y5, Y3, Y3
        VPADDD       Y3, Y1, Y5
        VPCMPGTD     max_write_d<>+0(SB), Y5, Y7
        VPTEST       Y7, Y7
        JNZ          long_codes
        VPSLLVD      Y1, Y2, Y2
        VPXOR        Y2, Y0, Y0
        VPBLENDD     $0x55, Y0, Y7, Y4
        VPSRLQ       $0x20, Y0, Y0
        VPSRLQ       $0x20, Y5, Y1
        VPBLENDD     $0x55, Y5, Y7, Y5
        VPSLLVQ      Y12, Y4, Y4
        VPXOR        Y11, Y4, Y4
        VPADDQ       Y12, Y5, Y5
        VPSLLVQ      Y5, Y0, Y0
        VPXOR        Y4, Y0, Y0
        VPADDQ       Y5, Y1, Y1
        VPBLENDD     $0x33, Y0, Y7, Y2
        VPBLENDD     $0x33, Y1, Y7, Y3
        VPSRLDQ      $0x08, Y0, Y0
        VPSRLDQ      $0x08, Y1, Y1
        VPADDQ       Y3, Y1, Y1
        VPAND        Y13, Y1, Y12
        VPERMQ       $0xcf, Y12, Y12
        VPADDQ       Y12, Y3, Y3
        VPSLLVQ      Y12, Y2, Y2
        VMOVDQA      q_64<>+0(SB), Y7
        VPSUBQ       Y3, Y7, Y5
        VPSRLVQ      Y5, Y0, Y4
        VPSLLDQ      $0x08, Y4, Y4
        VPSLLVQ      Y3, Y0, Y0
        VPXOR        Y4, Y0, Y0
        VPXOR        Y2, Y0, Y0
        VMOVQ        X1, R9
        SHRQ         $0x03, R9
        VPADDQ       Y12, Y1, Y3
        VPSRLQ       $0x03, Y3, Y3
        VPSHUFB      Y3, Y0, Y2
        VPAND        bytes_mask<>+0(SB), Y2, Y2
        VEXTRACTI128 $0x01, Y2, X11
        VPTEST       min_write_mask<>+0(SB), Y3
        JZ           short_codes

short_codes_next:
        VPERMQ       $0x45, Y2, Y2
        VPOR         Y2, Y0, Y0
        VEXTRACTI128 $0x01, Y1, X3
        VMOVDQU      Y0, (DX)
        VPADDQ       Y3, Y1, Y1
        VMOVQ        X1, R10
        SHRQ         $0x03, R10
        VPAND        Y13, Y1, Y12
        VEXTRACTI128 $0x01, Y0, (DX)(R9*1)
        ADDQ         R10, DX
        CMPQ         DI, R8
        JBE          main_loop

main_loop_exit:
        VMOVQ X12, CX
        VMOVQ X11, SI
        JMP   finish

short_codes:
        VPOR Y2, Y11, Y11
        JMP  short_codes_next

long_codes:
        ADDQ         $0x00000018, BX
        SUBQ         $0x00000020, DI
        VPXOR        Y7, Y7, Y7
        VPBLENDD     $0x55, Y0, Y7, Y4
        VPBLENDD     $0x55, Y1, Y7, Y5
        VPBLENDD     $0x55, Y2, Y7, Y6
        VPSLLVQ      Y5, Y6, Y6
        VPXOR        Y6, Y4, Y4
        VPADDD       Y3, Y1, Y5
        VPSRLQ       $0x20, Y0, Y0
        VPSRLQ       $0x20, Y1, Y1
        VPSRLQ       $0x20, Y2, Y2
        VPSLLVQ      Y1, Y2, Y2
        VPXOR        Y2, Y0, Y0
        VPSRLQ       $0x20, Y5, Y1
        VPBLENDD     $0x55, Y5, Y7, Y5
        VPSLLVQ      Y12, Y4, Y4
        VPXOR        Y11, Y4, Y4
        VPADDQ       Y12, Y5, Y5
        VPADDQ       Y5, Y1, Y1
        XORQ         SI, SI
        XORQ         CX, CX
        VPSUBQ       Y5, Y1, Y1
        CMPQ         DX, BX
        JA           overflow
        VMOVQ        X4, R9
        VMOVQ        X5, R10
        SHLXQ        CX, R9, R9
        ORQ          R9, SI
        ADDQ         R10, CX
        MOVQ         SI, (DX)
        MOVQ         CX, R9
        SHRQ         $0x03, R9
        ADDQ         R9, DX
        MOVQ         CX, R9
        ANDQ         $0xfffffff8, CX
        SHRXQ        CX, SI, SI
        MOVQ         R9, CX
        ANDQ         $0x07, CX
        ADDQ         $0x04, DI
        CMPQ         DX, BX
        JA           overflow
        VMOVQ        X0, R9
        VMOVQ        X1, R10
        SHLXQ        CX, R9, R9
        ORQ          R9, SI
        ADDQ         R10, CX
        MOVQ         SI, (DX)
        MOVQ         CX, R9
        SHRQ         $0x03, R9
        ADDQ         R9, DX
        MOVQ         CX, R9
        ANDQ         $0xfffffff8, CX
        SHRXQ        CX, SI, SI
        MOVQ         R9, CX
        ANDQ         $0x07, CX
        ADDQ         $0x04, DI
        CMPQ         DX, BX
        JA           overflow
        VPEXTRQ      $0x01, X4, R9
        VPEXTRQ      $0x01, X5, R10
        SHLXQ        CX, R9, R9
        ORQ          R9, SI
        ADDQ         R10, CX
        MOVQ         SI, (DX)
        MOVQ         CX, R9
        SHRQ         $0x03, R9
        ADDQ         R9, DX
        MOVQ         CX, R9
        ANDQ         $0xfffffff8, CX
        SHRXQ        CX, SI, SI
        MOVQ         R9, CX
        ANDQ         $0x07, CX
        ADDQ         $0x04, DI
        CMPQ         DX, BX
        JA           overflow
        VPEXTRQ      $0x01, X0, R9
        VPEXTRQ      $0x01, X1, R10
        SHLXQ        CX, R9, R9
        ORQ          R9, SI
        ADDQ         R10, CX
        MOVQ         SI, (DX)
        MOVQ         CX, R9
        SHRQ         $0x03, R9
        ADDQ         R9, DX
        MOVQ         CX, R9
        ANDQ         $0xfffffff8, CX
        SHRXQ        CX, SI, SI
        MOVQ         R9, CX
        ANDQ         $0x07, CX
        ADDQ         $0x04, DI
        VEXTRACTI128 $0x01, Y4, X4
        VEXTRACTI128 $0x01, Y5, X5
        VEXTRACTI128 $0x01, Y0, X0
        VEXTRACTI128 $0x01, Y1, X1
        CMPQ         DX, BX
        JA           overflow
        VMOVQ        X4, R9
        VMOVQ        X5, R10
        SHLXQ        CX, R9, R9
        ORQ          R9, SI
        ADDQ         R10, CX
        MOVQ         SI, (DX)
        MOVQ         CX, R9
        SHRQ         $0x03, R9
        ADDQ         R9, DX
        MOVQ         CX, R9
        ANDQ         $0xfffffff8, CX
        SHRXQ        CX, SI, SI
        MOVQ         R9, CX
        ANDQ         $0x07, CX
        ADDQ         $0x04, DI
        CMPQ         DX, BX
        JA           overflow
        VMOVQ        X0, R9
        VMOVQ        X1, R10
        SHLXQ        CX, R9, R9
        ORQ          R9, SI
        ADDQ         R10, CX
        MOVQ         SI, (DX)
        MOVQ         CX, R9
        SHRQ         $0x03, R9
        ADDQ         R9, DX
        MOVQ         CX, R9
        ANDQ         $0xfffffff8, CX
        SHRXQ        CX, SI, SI
        MOVQ         R9, CX
        ANDQ         $0x07, CX
        ADDQ         $0x04, DI
        CMPQ         DX, BX
        JA           overflow
        VPEXTRQ      $0x01, X4, R9
        VPEXTRQ      $0x01, X5, R10
        SHLXQ        CX, R9, R9
        ORQ          R9, SI
        ADDQ         R10, CX
        MOVQ         SI, (DX)
        MOVQ         CX, R9
        SHRQ         $0x03, R9
        ADDQ         R9, DX
        MOVQ         CX, R9
        ANDQ         $0xfffffff8, CX
        SHRXQ        CX, SI, SI
        MOVQ         R9, CX
        ANDQ         $0x07, CX
        ADDQ         $0x04, DI
        CMPQ         DX, BX
        JA           overflow
        VPEXTRQ      $0x01, X0, R9
        VPEXTRQ      $0x01, X1, R10
        SHLXQ        CX, R9, R9
        ORQ          R9, SI
        ADDQ         R10, CX
        MOVQ         SI, (DX)
        MOVQ         CX, R9
        SHRQ         $0x03, R9
        ADDQ         R9, DX
        MOVQ         CX, R9
        ANDQ         $0xfffffff8, CX
        SHRXQ        CX, SI, SI
        MOVQ         R9, CX
        ANDQ         $0x07, CX
        ADDQ         $0x04, DI
        VEXTRACTI128 $0x01, Y4, X4
        VEXTRACTI128 $0x01, Y5, X5
        VEXTRACTI128 $0x01, Y0, X0
        VEXTRACTI128 $0x01, Y1, X1
        VMOVQ        SI, X11
        VMOVQ        CX, X12
        SUBQ         $0x00000018, BX
        CMPQ         DI, R8
        JBE          main_loop

finish:
overflow:
        MOVQ buf+32(FP), AX
        MOVQ SI, 32(AX)
        MOVQ CX, 40(AX)
        MOVQ (AX), CX
        SUBQ CX, DX
        MOVQ DX, 24(AX)
        MOVQ tokens_base+8(FP), AX
        SUBQ AX, DI
        SHRQ $0x02, DI
        MOVQ DI, ret+40(FP)
        RET

skip:
        MOVQ $0x00000000, AX
        MOVQ AX, ret+40(FP)
        RET
